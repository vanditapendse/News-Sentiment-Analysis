{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.20.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests) (2018.10.15)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests) (2.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests) (1.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/DJ/Desktop/Pangaea/pangaea_key.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ad546e8cceae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m#der_spiegel(directory, keyword_articles) #INDEX = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;31m#cnn_middle_east(directory, keyword_articles)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0maljazeera_middle_east\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0mfox_news_middle_east\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ad546e8cceae>\u001b[0m in \u001b[0;36maljazeera_middle_east\u001b[0;34m(directory, keyword_articles)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mkeyword_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mlocal_kwa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mgraph_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_kwa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Aljazeera Middle East\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcnn_middle_east\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword_articles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f1add3e25d36>\u001b[0m in \u001b[0;36mgraph_sentiment\u001b[0;34m(keyword_articles, title)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice_account\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_service_account_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/DJ/Desktop/Pangaea/pangaea_key.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageServiceClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/oauth2/service_account.py\u001b[0m in \u001b[0;36mfrom_service_account_file\u001b[0;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[1;32m    208\u001b[0m         info, signer = _service_account_info.from_filename(\n\u001b[0;32m--> 209\u001b[0;31m             filename, require=['client_email', 'token_uri'])\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_signer_and_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google/auth/_service_account_info.py\u001b[0m in \u001b[0;36mfrom_filename\u001b[0;34m(filename, require)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0msigner\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/DJ/Desktop/Pangaea/pangaea_key.json'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "!pip install requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "directory = set()\n",
    "keyword_articles = set()\n",
    "#key_word = [\"kill\", \"saudi\", \"iran\"]\n",
    "key_word = \"saudi\"\n",
    "title_set = set()\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, title, link, score, magnitude, weighted):\n",
    "        self.title = title\n",
    "        self.link = link\n",
    "        self.score = score\n",
    "        self.magnitude = magnitude\n",
    "        self.weighted = weighted\n",
    "\n",
    "def bbc_middleeast(directory, keyword_articles): \n",
    "    local_kwa = set()\n",
    "    response = requests.get('https://www.bbc.com/news/world/middle_east')\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    headlines = doc.find_all('div')\n",
    "    for headline in headlines:\n",
    "        for a in headline.findAll('a'):\n",
    "            href = a.get('href')\n",
    "            title = a.text\n",
    "            link = 'https://www.bbc.com' + href\n",
    "            a = Article(title, link, None, None, None)\n",
    "            if a != None:\n",
    "                if(title not in title_set):\n",
    "                    title_set.add(title)\n",
    "                    directory.add(a)\n",
    "                    \n",
    "                    \n",
    "    for article in directory:\n",
    "        search = re.search(\"(saudi)\", (article.title).lower())\n",
    "        if search != None:\n",
    "            local_kwa.add(article)\n",
    "            keyword_articles.add(article)\n",
    "    \n",
    "    graph_sentiment(local_kwa, \"BBC Middle East\")\n",
    "def der_spiegel(directory, keyword_articles):\n",
    "    local_kwa = set()\n",
    "    response = requests.get('http://www.spiegel.de/international/')\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    headlines = doc.find_all('a')\n",
    "    for headline in headlines:\n",
    "        if(headline.get('href') != None and headline.text != None):\n",
    "            href = headline.get('href')\n",
    "            if(href[0:4] != 'http'):\n",
    "                link = 'http://www.spiegel.de' + href\n",
    "            else:\n",
    "                line = href\n",
    "            title = headline.text\n",
    "            a = Article(title, link, None, None, None)\n",
    "            if(a != None):\n",
    "                if(title not in title_set):\n",
    "                    title_set.add(title)\n",
    "                    directory.add(a)\n",
    "    \n",
    "    for article in directory:\n",
    "        search = re.search(\"(saudi)\", (article.title).lower())\n",
    "        if search != None:\n",
    "            keyword_articles.add(article)\n",
    "            local_kwa.add(article)\n",
    "    graph_sentiment(local_kwa, \"Der Spiegel\")\n",
    "    \n",
    "def aljazeera_middle_east(directory, keyword_articles):\n",
    "    local_kwa = set()\n",
    "    response = requests.get('https://www.aljazeera.com/topics/country/saudi-arabia.html')\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    headlines = doc.find_all('div')\n",
    "    for headline in headlines:\n",
    "        for a in headline.findAll('a'):\n",
    "            href = a.get('href')\n",
    "            title = a.text\n",
    "            if title != None:\n",
    "                if(href != '#'):\n",
    "                    link = 'https://www.aljazeera.com/' + href\n",
    "                    a = Article(title, link, None, None, None)\n",
    "                    if a != None:\n",
    "                        if(title not in title_set):\n",
    "                            title_set.add(title)\n",
    "                            directory.add(a)\n",
    "    for article in directory:\n",
    "        search = re.search(\"(saudi)\", (article.title).lower())\n",
    "        if search != None:\n",
    "            keyword_articles.add(article)\n",
    "            local_kwa.add(article)\n",
    "    graph_sentiment(local_kwa, \"Aljazeera Middle East\")\n",
    "\n",
    "def cnn_middle_east(directory, keyword_articles):\n",
    "    local_kwa = set()\n",
    "    response = requests.get('https://www.cnn.com/middle-east')\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    headlines = doc.find_all('a')\n",
    "    for headline in headlines:\n",
    "        href = headline.get('href')\n",
    "        for s1 in headline.findAll('span'):\n",
    "            title = s1.text\n",
    "            if title != None:\n",
    "                link = 'https://www.cnn.com' + href\n",
    "                a = Article(title, link, None, None, None)\n",
    "                if(title not in title_set):\n",
    "                    title_set.add(title)\n",
    "                    directory.add(a)\n",
    "    for article in directory:\n",
    "        search = re.search(\"(saudi)\", (article.title).lower())\n",
    "        if search != None:\n",
    "            keyword_articles.add(article)\n",
    "            local_kwa.add(article)\n",
    "    graph_sentiment(local_kwa, \"CNN\")\n",
    "    \n",
    "def fox_news_middle_east(directory, keyword_articles):\n",
    "    local_kwa = set()\n",
    "    response = requests.get('https://www.foxnews.com/category/world/world-regions/middle-east')\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    headlines = doc.find_all('a')\n",
    "    for headline in headlines:\n",
    "        href = headline.get('href')\n",
    "        for s1 in headline.findAll('img'):\n",
    "            title = s1.get('alt')\n",
    "            if title != None:\n",
    "                link = 'https://www.foxnews.com/' + href\n",
    "                a = Article(title, link, None, None, None)\n",
    "                if(title not in title_set):\n",
    "                    title_set.add(title)\n",
    "                    directory.add(a)\n",
    "    \n",
    "    for article in directory:\n",
    "        search = re.search(\"(saudi)\", (article.title).lower())\n",
    "        if search != None:\n",
    "            keyword_articles.add(article)\n",
    "            local_kwa.add(article)\n",
    "    graph_sentiment(local_kwa, \"Fox News\")\n",
    "    \n",
    "    \n",
    "#bbc_middleeast(directory, keyword_articles) #INDEX = 3\n",
    "#der_spiegel(directory, keyword_articles) #INDEX = 1\n",
    "#cnn_middle_east(directory, keyword_articles)\n",
    "aljazeera_middle_east(directory, keyword_articles)\n",
    "fox_news_middle_east(directory, keyword_articles)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import os\n",
    "from google.oauth2 import service_account\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "\n",
    "def graph_sentiment(keyword_articles, title):\n",
    "    count = 0\n",
    "    used = [];\n",
    "    aggregate_weighted_score = 0\n",
    "    aggregate_sentiment_score = 0\n",
    "    aggregate_magnitude = 0\n",
    "    score = []\n",
    "    magnitude = []\n",
    "\n",
    "\n",
    "\n",
    "    for article in keyword_articles:\n",
    "        if article.link not in used:\n",
    "            count = count + 1\n",
    "            page = requests.get(article.link)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "            body = list(soup.children)[1]\n",
    "\n",
    "            #list(body.children)[1]\n",
    "\n",
    "            text = ''\n",
    "            for p in soup.find_all('p'):\n",
    "                if p.get('class') == None:\n",
    "                    text = text + p.get_text()\n",
    "            credentials = service_account.Credentials.from_service_account_file(\"/Users/Vandita/Desktop/MSC/Pangaea/pangaea_key.json\")\n",
    "            client = language.LanguageServiceClient(credentials=credentials)\n",
    "\n",
    "            document = types.Document(content=text,type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "            sentiment = client.analyze_sentiment(document=document).document_sentiment\n",
    "    #         print(article.title)\n",
    "    #         print(article.link)\n",
    "    #         print('Sentiment: {}, {}'.format(sentiment.score, sentiment.magnitude))\n",
    "    #         total_sentiment_score += sentiment.score\n",
    "    #         total_sentiment_magnitude += sentiment.magnitude\n",
    "\n",
    "\n",
    "            weighted_sentiment = sentiment.score * sentiment.magnitude\n",
    "            \n",
    "            article.score = sentiment.score\n",
    "            article.magnitude = sentiment.magnitude\n",
    "            article.weighted = weighted_sentiment\n",
    "            \n",
    "            score.append(sentiment.score)\n",
    "            magnitude.append(sentiment.magnitude)\n",
    "            aggregate_sentiment_score += sentiment.score\n",
    "            aggregate_magnitude += sentiment.magnitude\n",
    "            aggregate_weighted_score += weighted_sentiment\n",
    "            #print(article.title, article.link, article.score, article.magnitude, article.weighted)\n",
    "            used.append(article.link);\n",
    "            \n",
    "    get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    plt.scatter(score, magnitude)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print(\"Sentiment Score:\", aggregate_sentiment_score)\n",
    "    print(\"Sentiment Magnitude\", aggregate_magnitude)\n",
    "    print(\"Aggregate Weighted Sentiment Score:\", aggregate_weighted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
